This is an ETL pipeline that extracts song and log data from an S3 bucket, processes the data using Spark, and loads the data back into an S3 bucket as a set of dimensional tables written as parquet files.

## The Datasets
there are two datasets both in json format

### Song Dataset
a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
Containing the following columns:
> "num_songs", "artist_id", "artist_latitude", "artist_longitude", "artist_location", "artist_name", "song_id", "title", "duration", "year"

### Log Dataset
datset generated by [event simulator](https://github.com/Interana/eventsim) based on the songs in the song dataset.
Containing the following columns:
> "artist", "auth", "firstName", "gender", "itemInSession", "lastName", "length", "level", "location", "method", "page", "registration", "sessionId", "song", "status", "ts", "userAgent", "userId"

This data is loaded into parquet files using the following star shcema.

## Star Schema Analytical Tables

### Fact table
**songplays** - created using staging_events and staging_songs data
> songplay_id, start_time, userId, level, song_id, artist_id, sessionId, location, user_agent

### Dimension tables

1. **users** - created using staging_events data
> userId, firstName, lastName, gender, level

2. **songs** - created using staging_songs data
> song_id, title, artist_id, year, duration

3. **artists** - created using staging_songs data
> artist_id, artist_name, artist_location, artist_lattitude, artist_longitude

4. **time** - crated using staging_events data
> start_time, hour, day, week, month, year, weekday

## The ETL Pipeline
The etl pipeline consist of one python script: **etl.py**.

The script depends on **dl.cfg** file. This file contains information related to the AWS. Specifically two pieces of information: **AWS credentials** and **s3 Bucket locations**

1. **AWS Credentials** - This consit of a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. You must obtain this from you own AWS account they are needed to access the S3 buckets os and copy from and into the S3 buckets. The best to get this is by creating an IAM User with the appropriate permissions.

2. **S3 bucket locations** - This cosist of an INPUT_PATH and an OUTPUT_PATH. The input path is already on the file as this is the location of the data you will read and process with spark. The OUTPUt_PATH will have to be that of a bucket you create with your AWS account which you have permissions to write to.

### etl.py
This script connects to the INPUT_PATH S3 bucket, read the data song data set and log data set in this buckets using spark processes the data and and creates the tables described by writing parquet files to the OUTPUTH_PATH S3 bucket. The files are placed under folder names corresponding to the table names. If the parquet file already exist, the scrip deletes it and creates a new one.

To run the script you will need to have the **dl.cfg** AWSCREDENTIALS and OUTINFOLDERS sections filled out. Then simply run with:
```
python3 etl.py
```

